Abstract:
  JSON is increasly used not only as a data transfer format, but as a data storage format as well
    With more JSON being stored, database users hope to execute complex OLAP queries without first converting their stored data to a relational structure or normalizing it
  This study explores whether it is possible to express complex queries on intricate JSON documents, and the performance implications of doing so
    It examines several systems, including PostgreSQL's binary JSON format, MongoDB, and a traditional normalized relational Postgres schema
  Finds that JSON-based systems are competitive, but that performance varies dramatically from queyr to query

Show example doc & queries

Introduction
Background
  JSON avoids object-relational impedence mismatch [Argo]
    Schemaless nature is attractive, as it no longer requires up-front schema design, and makes schema evolution easy
    Thus, data exchange format, input to DB, and result from DB can all be JSON

  Goal of this project:
    Understand whether it's possible to efficiently query JSON documents using complex queries, without having to decompose and normalize the data

    JSON documents thus designed (CL1) are complex and partially denormalized, featuring cast, reviews, and other information
      Models scenario in which external data source serves this JSON document as a whole, listing all known informatoin about movie
      Mirrors sources like:
        The Movie Database: http://docs.themoviedb.apiary.io/#reference/movies/movieid/get
        Rotten Tomatoes API: http://developer.rottentomatoes.com/docs/json/v10/Movie_Info
    This reflects use case of JSON in the wild -- store large documents with complex nested substructure
      Better reflects use cae than queries and documents used in Argo

  Postgres introduced initial support in 9.3, extended it with jsonb in 9.4
    Mirror the storage/query/index principles outlined for Oracle's scheme
      Storage: don't shred documents to store them relationally -- store natively
        Allow querying common attributes, despite lack of schema
      Qutery principle: use SQL, extended with JSON-specific operators implementing means of navigating & querying JSON docs
      Index principles: use JSON inverted index to support efficient querying

  As with Oracle,t his brings advantage of easy integration into existing environments
    Easier to combine data from multiple sources

  Other systems considered:
    Argo, but not rich enough
    Oracle, but no free version
    ToroDB is new system storing JSON data in Postgres using relational method
      Notably, it speaks the MongoDB wire protocol, and so promises easy transition
      Only two of my six queries succeeded, however

Methods
Results & Discussion
Conclusion & future work

Distribution of data (e.g., movie ratings) doesn't reflect what you'd observe in the real world


Q3:

Only queries that take > 4 s on slowest:
  Q3 (Mongo still under 5 s)
  Q5 (Mongo ~5 s)

Alternatives considered:
  Argo
    Insufficiently rich to exprses queries
  Oracle
    No freely available test versoin that supports JSON storage available
  Toro
    Failed on everything except Q1, Q2, insert

Can partition queries into two sets
  Q1, Q2, Q4: fast
  Q3, Q5, Q6: slow


Queries:
  q1: What movies were released in the last year?
  q2: What movies are in both the "action" and "adventure" genres?
  q3: What is the average rating of each movie?
  q4: What movies contained given actor?
  q5: What movies have at least half of their reviewers from Canada?
  q6: What movies share at least one actor?
    Mongo: While Argo authors used map-reduce, that's no longer possible with switch to V8 -- must use combo of two queries

Implementation:
  Mongo: Q1, Q2, Q4 are straightforward -- just take find()
    Q3 and Q5 use aggregation framework -- reputedly faster than map-reduce

Three classes:
  Q1 to Q5
  Q6
  Insert

Future work: extend Argo -- would it perform well with richer query language?
  Mongo only pulled ahead for massive data sets -- what is threshold for more complex docs?
Make Toro better

Background:
  Argo lacked the expressive power necessary to express my desired queries

Conclusion:
  All three systems are suitable for OLAP
    All three are rich enough (unlike Argo) and mature enough (unlike Toro) to express complex queries on complex documents
    PGSJ is uniformly poor performer -- for first four queries, pay penalties of 1065x, 25x, 28x, 65x relative to fastest system
      Inserts pay penalty of 7x

  Insert times slower in Postgres due to less efficient index generation
    Exlucding insert times, performance of Mongo vs. PGSR almost same on Q1-Q6Q (109.0 s vs. 109.5 s)

  PR is generally fastest system
  Mongo was fastest, oddly for the two slowest queries, and in second place for the other three
    Significant Mongo edge for Q1 and Q4 (77x, 7.3x); much more meager for Q2 and Q4
    Postgres-JSON's numbers are uniformly poor

  Good performance in PGSR does not guarantee the same in PGSJ, despite use of efficient indices
  MGDB may be faster for aggreation

Conclusion:
  Mongo and Postgres both have sufficient expressive power to represnet complex queries on JSON
    This is despite offering substantially different computational models -- Postgres is quite similar to the traditional relational model, while Mongo allows moderately complex queries to be expressed via aggregation framework, but requires ad hoc procedural code to be written for more compelx yet

  PGSJ is slow, but relatively immature
    Mongo is fast enough to be competitive with relational systems
    Performance differs on query-by-query basis -- so, you ust benchmark for your particular workflow
  PGSJ may evolve to be faster, which would bring advantage of not needing separate system
    Interesting would be extending Argo -- given richer query language, maybe its practice of shredding JSON would be beneficial, at least for smaller sets of documents as per its paper
    Equally interesting would be extending Toro to support full range of Mongo queries -- authors show it being significantly faster than Mongo, with easy migration for existing Mongo apps

  Ultimately, issuing complex queries on denormalized JSON docs is realistic
    These can be stored directly from producers, without first shredding them into relational systems
    PGSJ and Oracle show traditional DBMSes taking on JSON roles, replacing specialized stores like Mongo and eliminating need for middleware such as Argo or Toro
    So, relational DBMSes will likely continue evolving to support changes in storage models -- amazingly versatile systems





Results: (from OS X -- no longer accurate)
  q1: postgres_relational_queries=0.00232808347791      mongo_queries=0.180423054309                 postgres_json_queries=1.98368090345       (77.498533029522903,  10.994608815637593)
  q2: postgres_relational_queries=0.0949875869788       mongo_queries=0.157480656798                 postgres_json_queries=1.91000395142       (1.6579077520214009,  12.128498764611567)
  q3: mongo_queries=4.43454007304                       postgres_relational_queries=7.017947307      postgres_json_queries=63.0984412589       (1.5825648638660861,  8.9910109749591243)
  q4: postgres_relational_queries=0.0726058978587       mongo_queries=2.29704796679                  postgres_json_queries=3.37842884019       (31.637209022022287,  1.4707698267695177)
  q5: mongo_queries=5.1462759679                        postgres_json_queries=37.8945201             postgres_relational_queries=48.393649588  (7.3634838738492778,  1.2770619461677941)
  insert: mongo_queries=116.1146                        postgres_relational_queries=243.4789         postgres_json_queries=290.7325            (2.0968844572517154,  1.1940767762627478)

  Totals (without insert or Q6):
    mongo_queries=12.2157677188
    postgres_relational_queries=55.5815184633
    postgres_json_queries=108.265075054
